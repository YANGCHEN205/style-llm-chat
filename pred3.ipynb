{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2024/6/16 20:50\n",
    "# @Author  : yblir\n",
    "# @File    : lyb_lora_inference.py\n",
    "# explain  : \n",
    "# =======================================================\n",
    "import yaml\n",
    "import json\n",
    "from loguru import logger\n",
    "import time\n",
    "import sys\n",
    "from src.llamafactory.chat import ChatModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2108] 2024-08-30 14:10:50,934 >> loading file qwen.tiktoken from cache at /root/autodl-tmp/hug/models--Qwen--Qwen-7B-chat/snapshots/93a65d34827a3cc269b727e67004743b723e2f83/qwen.tiktoken\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-08-30 14:10:50,936 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-08-30 14:10:50,937 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-08-30 14:10:50,939 >> loading file tokenizer_config.json from cache at /root/autodl-tmp/hug/models--Qwen--Qwen-7B-chat/snapshots/93a65d34827a3cc269b727e67004743b723e2f83/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-08-30 14:10:50,940 >> loading file tokenizer.json from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/30/2024 14:10:51 - INFO - src.llamafactory.data.template - Add eos token: <|endoftext|>\n",
      "08/30/2024 14:10:51 - INFO - src.llamafactory.data.template - Add pad token: <|endoftext|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-08-30 14:10:51,856 >> loading configuration file config.json from cache at /root/autodl-tmp/hug/models--Qwen--Qwen-7B-chat/snapshots/93a65d34827a3cc269b727e67004743b723e2f83/config.json\n",
      "[INFO|configuration_utils.py:733] 2024-08-30 14:10:52,910 >> loading configuration file config.json from cache at /root/autodl-tmp/hug/models--Qwen--Qwen-7B-chat/snapshots/93a65d34827a3cc269b727e67004743b723e2f83/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-08-30 14:10:52,914 >> Model config QWenConfig {\n",
      "  \"_name_or_path\": \"Qwen/Qwen-7B-chat\",\n",
      "  \"architectures\": [\n",
      "    \"QWenLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_dropout_prob\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"Qwen/Qwen-7B-chat--configuration_qwen.QWenConfig\",\n",
      "    \"AutoModelForCausalLM\": \"Qwen/Qwen-7B-chat--modeling_qwen.QWenLMHeadModel\"\n",
      "  },\n",
      "  \"bf16\": false,\n",
      "  \"emb_dropout_prob\": 0.0,\n",
      "  \"fp16\": false,\n",
      "  \"fp32\": false,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 22016,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"qwen\",\n",
      "  \"no_bias\": true,\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"onnx_safe\": null,\n",
      "  \"rotary_emb_base\": 10000,\n",
      "  \"rotary_pct\": 1.0,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"softmax_in_fp32\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"QWenTokenizer\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_cache_kernel\": false,\n",
      "  \"use_cache_quantization\": false,\n",
      "  \"use_dynamic_ntk\": true,\n",
      "  \"use_flash_attn\": \"auto\",\n",
      "  \"use_logn_attn\": true,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/30/2024 14:10:52 - INFO - src.llamafactory.model.patcher - Using KV cache for faster generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3474] 2024-08-30 14:10:53,488 >> loading weights file model.safetensors from cache at /root/autodl-tmp/hug/models--Qwen--Qwen-7B-chat/snapshots/93a65d34827a3cc269b727e67004743b723e2f83/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1519] 2024-08-30 14:10:53,491 >> Instantiating QWenLMHeadModel model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:962] 2024-08-30 14:10:53,493 >> Generate config GenerationConfig {}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f274f7d534b4da9a7fc95c14d9acaaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:4280] 2024-08-30 14:11:41,962 >> All model checkpoint weights were used when initializing QWenLMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-08-30 14:11:41,964 >> All the weights of QWenLMHeadModel were initialized from the model checkpoint at Qwen/Qwen-7B-chat.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use QWenLMHeadModel for predictions without further training.\n",
      "[INFO|configuration_utils.py:917] 2024-08-30 14:11:42,541 >> loading configuration file generation_config.json from cache at /root/autodl-tmp/hug/models--Qwen--Qwen-7B-chat/snapshots/93a65d34827a3cc269b727e67004743b723e2f83/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-08-30 14:11:42,543 >> Generate config GenerationConfig {\n",
      "  \"chat_format\": \"chatml\",\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"max_new_tokens\": 512,\n",
      "  \"max_window_size\": 24000,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"top_k\": 0,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/30/2024 14:11:42 - INFO - src.llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
      "08/30/2024 14:11:42 - INFO - src.llamafactory.model.loader - all params: 7,721,324,544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2106] 2024-08-30 14:11:42,576 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-08-30 14:11:42,577 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-08-30 14:11:42,577 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-08-30 14:11:42,578 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-08-30 14:11:42,579 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-08-30 14:11:42,579 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-08-30 14:11:42,782 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:731] 2024-08-30 14:11:42,784 >> loading configuration file /root/autodl-tmp/hug/models--Qwen--Qwen1.5-0.5B-chat/snapshots/4d14e384a4b037942bb3f3016665157c8bcb70ea/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-08-30 14:11:42,785 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/root/autodl-tmp/hug/models--Qwen--Qwen1.5-0.5B-chat/snapshots/4d14e384a4b037942bb3f3016665157c8bcb70ea\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2816,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/30/2024 14:11:42 - INFO - src.llamafactory.model.patcher - Using KV cache for faster generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3471] 2024-08-30 14:11:42,790 >> loading weights file /root/autodl-tmp/hug/models--Qwen--Qwen1.5-0.5B-chat/snapshots/4d14e384a4b037942bb3f3016665157c8bcb70ea/model.safetensors\n",
      "[INFO|modeling_utils.py:1519] 2024-08-30 14:11:42,804 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:962] 2024-08-30 14:11:42,806 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4280] 2024-08-30 14:12:21,058 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-08-30 14:12:21,060 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /root/autodl-tmp/hug/models--Qwen--Qwen1.5-0.5B-chat/snapshots/4d14e384a4b037942bb3f3016665157c8bcb70ea.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:915] 2024-08-30 14:12:21,064 >> loading configuration file /root/autodl-tmp/hug/models--Qwen--Qwen1.5-0.5B-chat/snapshots/4d14e384a4b037942bb3f3016665157c8bcb70ea/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-08-30 14:12:21,065 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/30/2024 14:12:21 - INFO - src.llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "08/30/2024 14:12:21 - INFO - src.llamafactory.model.adapter - Merged 1 adapter(s).\n",
      "08/30/2024 14:12:21 - INFO - src.llamafactory.model.adapter - Loaded adapter(s): //root/autodl-tmp/LLaMA-Factory/roleplay/output/Qwen1.5-0.5B-chat/SanGuoYanyi_Parallel\n",
      "08/30/2024 14:12:21 - INFO - src.llamafactory.model.loader - all params: 463,987,712\n"
     ]
    }
   ],
   "source": [
    "with open('/root/autodl-tmp/LLaMA-Factory/roleplay/chat_model.yaml', 'r', encoding='utf-8') as f:\n",
    "    param = yaml.safe_load(f)\n",
    "\n",
    "chat_model = ChatModel(param)\n",
    "\n",
    "\n",
    "with open('/root/autodl-tmp/LLaMA-Factory/roleplay/style_model.yaml', 'r', encoding='utf-8') as f:\n",
    "    param = yaml.safe_load(f)\n",
    "\n",
    "style_model = ChatModel(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat reponse:   学习是一个持续的过程，需要我们持之以恒地努力。建议大家首先要明确自己的学习目标，然后制定一个科学的学习计划，并且要有耐心和毅力。此外，学习过程中要注意培养良好的学习习惯，如定期复习、及时总结、多思考等，这些都能帮助我们更好地掌握知识。最后，大家要多与老师和同学交流，共同学习，共同进步。希望以上建议能对大家有所帮助。\n",
      "'t\n",
      "\n",
      "style reponse:  学无止境，须有恒心。建议诸君先明学习目标，制定科学学习计划，要耐劳有毅力。其次，要养成良好的学习习惯，如经常复习、及时总结、多思考等。此乃学之大法也。希望诸君遵此。\n"
     ]
    }
   ],
   "source": [
    "# # 预热\n",
    "\n",
    "# prompt =\"严冬时节，鹅毛一样的大雪片在天空中到处飞舞着，有一个王后坐在王宫里的一扇窗子边，正在为她的女儿做针线活儿，寒风卷着雪片飘进了窗子，乌木窗台上飘落了不少雪花。\"  \n",
    "prompt = \"请扮演鲁智深回答以下问题：“给出一些学习的建议”\" \n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "res = chat_model.chat(messages)\n",
    "print(\"chat reponse: \",res[0].response_text)\n",
    "\n",
    "prompt = res[0].response_text\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "res = style_model.chat(messages)\n",
    "print(\"style reponse: \", res[0].response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "style reponse:  严冬时日，大雪纷飞，有一王后坐于宫中一扇窗下，正在为儿做针线，寒风卷起大雪，飘入窗中。\n"
     ]
    }
   ],
   "source": [
    "# # 预热\n",
    "prompt =\"严冬时节，鹅毛一样的大雪片在天空中到处飞舞着，有一个王后坐在王宫里的一扇窗子边，正在为她的女儿做针线活儿，寒风卷着雪片飘进了窗子，乌木窗台上飘落了不少雪花。\"  \n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "res = style_model.chat(messages)\n",
    "print(\"style reponse: \", res[0].response_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
