import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
import sys
import fire
import gradio as gr
from gradio import utils
import torch
from peft import PeftModel
from transformers import GenerationConfig, AutoTokenizer, AutoModel
import yaml
import json
from loguru import logger
import time
import sys
from src.llamafactory.chat import ChatModel


if torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"

try:
    if torch.backends.mps.is_available():
        device = "mps"
except:
    pass


class Chat_Model(object):
    def __init__(self, scene=None, character=None, config_path="/root/autodl-tmp/LLaMA-Factory/roleplay/single_chat_model.yaml"):
        self.device = device
        self.scene = scene
        self.config_path = config_path
        self.character = character
        self.model = self.prepare_model(self.config_path)

    def prepare_model(self, config_path):
        # load model
        with open(config_path, 'r', encoding='utf-8') as f:
            chat_model_param = yaml.safe_load(f)
        chat_model = ChatModel(chat_model_param)
        return chat_model

    # def get_headers(self):
    #     if self.scene and self.character:
    #         headers = [
    #             {"role": "user", "content": f"è¯·æ‰®æ¼”{self.scene}ä¸­çš„{self.character}ã€‚ä½¿ç”¨ç°ä»£æ–‡é£æ™®é€šè¯ï¼Œç”¨{self.character}ï¼š...æ ¼å¼è¿›è¡Œå›å¤,"},
    #         ]
    #     else:
    #         headers = []
    #     return headers

    def generate(self, messages, temperature=0.9, top_p=0.75, top_k=40, num_beams=1, do_sample=True, max_new_tokens=128):
        conversation = messages
        chat_model_res = self.model.chat(
            messages=conversation,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            num_beams=num_beams,
            do_sample=do_sample,
            return_dict_in_generate=True,
            output_scores=True,
            max_new_tokens=max_new_tokens,
        )
        return chat_model_res


def main(
    server_name: str = "0.0.0.0",
    share_gradio: bool = False,
    chat_model_config_path="/root/autodl-tmp/LLaMA-Factory/roleplay/single_chat_model.yaml"):

    chat_model = Chat_Model(scene=None, character=None, config_path=chat_model_config_path)
    
    def evaluate(
        input_text="",
        do_sample=False,
        temperature=0.1,
        top_p=0.75,
        top_k=40,
        num_beams=4,
        max_new_tokens=128,
        history=[],
        max_history=0,
        **kwargs,
    ):

        context = input_text
        if max_history > 0 and history:
            tem_history = sum(history, [])
            context = "".join(tem_history[-max_history:]) + input_text
        chat_model_prompt = context


        messages = [{"role": "user", "content": chat_model_prompt}]
        chat_model_res = chat_model.generate(
            messages,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            num_beams=num_beams,
            do_sample=do_sample,
            max_new_tokens=max_new_tokens,
        )
        output = chat_model_res[0].response_text
        # output = 'werwerwer'
        history.append([input_text, output])
        return "", history

    with gr.Blocks() as demo:
        title = "ğŸ’¬Style-LLM-Chat-Single"
        description ="""
        Style-LLM-Chat-Single æ˜¯ä½¿ç”¨LLAMA-FACTORYæ¡†æ¶ä¸Šé‡‡ç”¨è‡ªå®šä¹‰å¤æ–‡é£æ ¼Â·æ•°æ®é›†,ä½¿ç”¨SFT,RM,PPOè¿›è¡Œå¾®è°ƒ,èƒ½å¤Ÿå®ç°åœ¨å•ä¸ªæ¨¡å‹çš„åŸºç¡€ä¸Šå®ç°å¤æ–‡é£æ ¼çš„å¯¹è¯ã€‚
        é¡¹ç›®æ›´å¤šè¯¦æƒ…è§ â€œhttps://github.com/YANGCHEN205/style-llm-chatâ€
        å‚æ•°è¯¦è§£ï¼š
        temperatureï¼ˆæ¸©åº¦ï¼‰: è¿™ä¸ªå‚æ•°ç”¨äºæ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ã€‚æ¸©åº¦è¾ƒä½ï¼ˆæ¯”å¦‚0.1ï¼‰ä¼šä½¿æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬æ¯”è¾ƒç¡®å®šæ€§å’Œä¸€è‡´æ€§å¼ºï¼Œè€Œè¾ƒé«˜çš„æ¸©åº¦ï¼ˆæ¯”å¦‚1.0æˆ–æ›´é«˜ï¼‰ä¼šä½¿æ–‡æœ¬æ›´å¤šæ ·åŒ–å’Œéšæœºã€‚
        top_pï¼ˆä¹Ÿç§°ä½œnucleus samplingï¼‰: è¿™ä¸ªå‚æ•°ç”¨äºæ§åˆ¶åœ¨ç”Ÿæˆæ¯ä¸€ä¸ªè¯æ—¶è€ƒè™‘çš„æ¦‚ç‡è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼Œtop_pä¸º0.9æ„å‘³ç€åªè€ƒè™‘ç´¯è®¡æ¦‚ç‡è‡³å°‘è¾¾åˆ°90%çš„è¯ã€‚è¿™æœ‰åŠ©äºå‰”é™¤é‚£äº›æå…¶ç½•è§çš„è¯ï¼Œä½¿ç”Ÿæˆçš„å†…å®¹æ›´åŠ æµç•…å’Œåˆç†ã€‚
        top_k: è¿™ä¸ªå‚æ•°é™åˆ¶äº†åœ¨ç”Ÿæˆæ¯ä¸ªè¯æ—¶è€ƒè™‘çš„å¯èƒ½è¯çš„æ•°é‡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœtop_kè®¾ä¸º50ï¼Œåˆ™æ¯æ¬¡ç”Ÿæˆè¯æ—¶åªä»æ¦‚ç‡æœ€é«˜çš„å‰50ä¸ªè¯ä¸­é€‰æ‹©ã€‚
        num_beamsï¼ˆæŸæœç´¢ï¼‰: è¿™ä¸ªå‚æ•°ç”¨äºæŸæœç´¢ï¼Œä¸€ç§ç”¨äºç”Ÿæˆæ›´åˆç†æ–‡æœ¬çš„æŠ€æœ¯ã€‚num_beamsè®¾ç½®ä¸ºå¤§äº1çš„å€¼æ—¶ï¼Œæ¨¡å‹å°†æ¢ç´¢å¤šç§å¯èƒ½çš„å¥å­ç»„åˆï¼Œé€‰æ‹©æ•´ä½“ä¸Šæœ€æœ‰å¯èƒ½ï¼ˆæˆ–å¾—åˆ†æœ€é«˜ï¼‰çš„è¾“å‡ºã€‚
        do_sample: è¿™ä¸ªå¸ƒå°”å‚æ•°ç¡®å®šæ˜¯å¦åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶è¿›è¡Œéšæœºé‡‡æ ·ã€‚å¦‚æœè®¾ç½®ä¸ºTrueï¼Œæ¨¡å‹åœ¨ç”Ÿæˆæ¯ä¸ªè¯æ—¶ä¼šåŸºäºä¿®æ”¹åçš„æ¦‚ç‡åˆ†å¸ƒè¿›è¡ŒæŠ½æ ·ï¼Œå¢åŠ æ–‡æœ¬çš„å¤šæ ·æ€§ã€‚
        return_dict_in_generate: å½“è¿™ä¸ªå‚æ•°ä¸ºTrueæ—¶ï¼Œæ¨¡å‹ç”Ÿæˆå‡½æ•°å°†è¿”å›ä¸€ä¸ªåŒ…å«æ›´å¤šä¿¡æ¯çš„å­—å…¸ï¼Œå¦‚ç”Ÿæˆçš„è¯çš„åˆ†æ•°ç­‰ï¼Œè€Œä¸ä»…ä»…æ˜¯æ–‡æœ¬ã€‚
        output_scores: å¦‚æœè®¾ç½®ä¸ºTrueï¼Œç”Ÿæˆçš„è¾“å‡ºå°†åŒ…æ‹¬é¢„æµ‹æ¯ä¸ªè¯çš„åˆ†æ•°ï¼ˆé€šå¸¸æ˜¯æ¦‚ç‡å¯¹æ•°ï¼‰ã€‚è¿™å¯¹äºç†è§£å’Œåˆ†ææ¨¡å‹çš„è¡Œä¸ºéå¸¸æœ‰ç”¨ã€‚
        max_new_tokens: è¿™ä¸ªå‚æ•°é™åˆ¶äº†ç”Ÿæˆçš„æœ€å¤§è¯ï¼ˆtokenï¼‰æ•°é‡ã€‚è¿™æœ‰åŠ©äºæ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„é•¿åº¦ã€‚
        ğŸ“•å¯¹è¯æ¨¡å‹:01ai-Yi1.5-6B-chat         ğŸ‘¨â€ğŸ¦²è§’è‰²ï¼šåˆ˜å¤‡
        """
        gr.Markdown("<h1 style='text-align: center; margin-bottom: 1rem'>" + title + "</h1>")
        gr.Markdown(description)  # ç›´æ¥ä¼ å…¥ Markdown æ–‡æœ¬

        with gr.Row():
            with gr.Column(scale=2):
                chatbot = gr.Chatbot(label="å¯¹è¯æ¡†")
                msg = gr.Textbox(label="è¾“å…¥æ¡†")
                with gr.Row():
                    clear = gr.Button("Clear")
                    retry = gr.Button("retry")
                    submit = gr.Button("Submit", variant="primary")
            with gr.Column(scale=1):
                do_sample = gr.inputs.Checkbox(label="Do sample")
                temperature = gr.components.Slider(minimum=0, maximum=1, value=0.1, label="Temperature")
                top_p = gr.components.Slider(minimum=0, maximum=1, value=0.75, label="Top p")
                top_k = gr.components.Slider(minimum=0, maximum=100, step=1, value=40, label="Top k")
                beams = gr.components.Slider(minimum=1, maximum=1, step=1, value=1, label="Beams(BUG)")
                max_tokens = gr.components.Slider(minimum=1, maximum=512, step=1, value=128, label="Max tokens")
                max_history = gr.components.Slider(minimum=0, maximum=20, step=1, value=10, label="Max history")

        submit.click(evaluate, [msg, do_sample, temperature, top_p, top_k, beams, max_tokens, chatbot, max_history], [msg, chatbot], queue=False)
        clear.click(lambda: "", None, chatbot, queue=False)
        retry.click(lambda v: (v[-1][0] if v else "", v[:-1]), chatbot, [msg, chatbot], queue=False)

    demo.title = title
    demo.launch(server_name=server_name, share=share_gradio)


if __name__ == "__main__":
    fire.Fire(main)