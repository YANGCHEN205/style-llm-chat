{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2024/6/16 20:50\n",
    "# @Author  : yblir\n",
    "# @File    : lyb_lora_inference.py\n",
    "# explain  : \n",
    "# =======================================================\n",
    "import yaml\n",
    "import json\n",
    "from loguru import logger\n",
    "import time\n",
    "import sys\n",
    "from src.llamafactory.chat import ChatModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # 1. 读取JSON数据\n",
    "# with open('/root/autodl-tmp/LLaMA-Factory/data/alpaca_gpt4_data_zh_modified.json', 'r', encoding='utf-8') as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# # 2. 统计output字段的长度\n",
    "# lengths = [len(item['output']) for item in data]\n",
    "\n",
    "# # 可视化长度分布\n",
    "# plt.hist(lengths, bins=20, edgecolor='black')\n",
    "# plt.title('Output Length Distribution')\n",
    "# plt.xlabel('Length')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "# # 3. 删除所有output长度大于100的记录\n",
    "# max_length_threshold = 650\n",
    "# data = [item for item in data if len(item['output']) <= max_length_threshold]\n",
    "\n",
    "# # 4. 保存修改后的JSON数据\n",
    "# with open('/root/autodl-tmp/LLaMA-Factory/data/alpaca_gpt4_data_zh_modified_del.json', 'w', encoding='utf-8') as file:\n",
    "#     json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# # 1. 读取JSON数据\n",
    "# with open('/root/autodl-tmp/LLaMA-Factory/data/alpaca_gpt4_data_zh_modified_del.json', 'r', encoding='utf-8') as file:\n",
    "#     data = json.load(file)\n",
    "\n",
    "# # 2. 统计output字段的长度\n",
    "# lengths = [len(item['output']) for item in data]\n",
    "\n",
    "# # 可视化长度分布\n",
    "# plt.hist(lengths, bins=20, edgecolor='black')\n",
    "# plt.title('Output Length Distribution')\n",
    "# plt.xlabel('Length')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()\n",
    "\n",
    "# print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2287] 2024-08-30 20:23:45,552 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2287] 2024-08-30 20:23:45,553 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2287] 2024-08-30 20:23:45,554 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2287] 2024-08-30 20:23:45,555 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2287] 2024-08-30 20:23:45,556 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2287] 2024-08-30 20:23:45,557 >> loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2533] 2024-08-30 20:23:45,783 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:731] 2024-08-30 20:23:45,785 >> loading configuration file /root/autodl-tmp/hug/models--Qwen--Qwen1.5-0.5B-chat/snapshots/4d14e384a4b037942bb3f3016665157c8bcb70ea/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-08-30 20:23:45,788 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/root/autodl-tmp/hug/models--Qwen--Qwen1.5-0.5B-chat/snapshots/4d14e384a4b037942bb3f3016665157c8bcb70ea\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2816,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.43.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/30/2024 20:23:45 - INFO - src.llamafactory.model.patcher - Using KV cache for faster generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3641] 2024-08-30 20:23:45,822 >> loading weights file /root/autodl-tmp/hug/models--Qwen--Qwen1.5-0.5B-chat/snapshots/4d14e384a4b037942bb3f3016665157c8bcb70ea/model.safetensors\n",
      "[INFO|modeling_utils.py:1572] 2024-08-30 20:23:45,831 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1038] 2024-08-30 20:23:45,835 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4473] 2024-08-30 20:24:40,706 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4481] 2024-08-30 20:24:40,708 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /root/autodl-tmp/hug/models--Qwen--Qwen1.5-0.5B-chat/snapshots/4d14e384a4b037942bb3f3016665157c8bcb70ea.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:991] 2024-08-30 20:24:40,711 >> loading configuration file /root/autodl-tmp/hug/models--Qwen--Qwen1.5-0.5B-chat/snapshots/4d14e384a4b037942bb3f3016665157c8bcb70ea/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-08-30 20:24:40,712 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/30/2024 20:24:40 - INFO - src.llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "08/30/2024 20:24:41 - INFO - src.llamafactory.model.adapter - Merged 1 adapter(s).\n",
      "08/30/2024 20:24:41 - INFO - src.llamafactory.model.adapter - Loaded adapter(s): //root/autodl-tmp/LLaMA-Factory/roleplay/output/Qwen1.5-0.5B-chat/SanGuoYanyi_Parallel\n",
      "08/30/2024 20:24:41 - INFO - src.llamafactory.model.loader - all params: 463,987,712\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# with open('/root/autodl-tmp/LLaMA-Factory/roleplay/chat_model.yaml', 'r', encoding='utf-8') as f:\n",
    "#     param = yaml.safe_load(f)\n",
    "\n",
    "# chat_model = ChatModel(param)\n",
    "\n",
    "\n",
    "with open('/root/autodl-tmp/LLaMA-Factory/roleplay/style_model.yaml', 'r', encoding='utf-8') as f:\n",
    "    param = yaml.safe_load(f)\n",
    "\n",
    "style_model = ChatModel(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贾珍在这里指示将刚刚准备好的物品留下一些供祖宗享用，然后取走一些其他物品交给贾蓉送到荣府，自己则留下一部分家中所需。剩余的部分被派出去，整齐地堆放在月台底下。贾珍还命人召来族中子侄，将这些物品公平地分给他们。\n",
    "严冬时节，鹅毛一样的大雪片在天空中到处飞舞着，有一个王后坐在王宫里的一扇窗子边，正在为她的女儿做针线活儿，寒风卷着雪片飘进了窗子，乌木窗台上飘落了不少雪花。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "style reponse:  严冬时分，大雪如鹅毛，飞舞于天，宫中一扇窗下，一女女正在绣针线，有寒风卷起雪花，入窗。\n"
     ]
    }
   ],
   "source": [
    "# # 预热\n",
    "\n",
    "prompt =\"严冬时节，鹅毛一样的大雪片在天空中到处飞舞着，有一个王后坐在王宫里的一扇窗子边，正在为她的女儿做针线活儿，寒风卷着雪片飘进了窗子，乌木窗台上飘落了不少雪花。\"  \n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "res = style_model.chat(messages,temperature=0.9,a=1)\n",
    "print(\"style reponse: \", res[0].response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/root/autodl-tmp/LLaMA-Factory/data/alpaca_gpt4_data_zh.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_data=data[0]['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "把以下文字中的中文部分改写成三国演义原著的古文行文风格：“以下是保持健康的三个提示：\n",
      "\n",
      "1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\n",
      "\n",
      "2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\n",
      "\n",
      "3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。” \n",
      "48818\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"把以下文字中的中文部分改写成三国演义原著的古文行文风格：“{prompt_data}” \" \n",
    "print(prompt)\n",
    "print(len(data))\n",
    "\"把以下文字改写成三国演义原著的古文行文风格,只需要最终的结果,不需要额外的添加内容：‘对不起，作为一个语言模型，我无法渲染三维模型。但我可以为您提供一些建议。\\n\\n在制作三维模型时，您需要使用专业的三维建模软件，例如 Blender、Maya 或 Sketchup。这些软件提供了丰富的建模工具，可以让您轻松创建三维模型。\\n\\n在渲染三维模型时，您可以使用光照和材质来增强模型的真实感。常见的渲染器有 Blender 的 Cycles 渲染器和 Maya 的 Arnold 渲染器，它们都能产生逼真的渲染结果。\\n\\n如果您是初学者，建议您搜索相关教程学习三维建模和渲染技术。这些技能可以让您更快地掌握如何渲染一座房子的三维模型。’ \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/root/autodl-tmp/LLaMA-Factory/data/alpaca_gpt4_data_zh.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "for i in range(10):\n",
    "    prompt_data=data[i]['output']\n",
    "    prompt = f\"把以下文字改写成三国演义原著的古文行文风格,只需要最终的结果：“{prompt_data}” \" \n",
    "    print(prompt)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    res = chat_model.chat(messages)\n",
    "    print(\"chat reponse: \",res[0].response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [3:59:37<00:00,  4.79s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换完成，结果已保存。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tqdm\n",
    "import random\n",
    "\n",
    "# 假设这是转换到古文的函数\n",
    "def convert_to_classical_chinese(text):\n",
    "    # 这里应该是将现代文本转换为古文的逻辑\n",
    "    # 例如，你可以用替换规则或者调用某个外部的API进行转换\n",
    "    prompt = f\"把以下文字改写成三国演义原著的古文行文风格,只需要最终的结果,不需要额外的添加内容：‘{text}’ \"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    res = chat_model.chat(messages)\n",
    "    return res[0].response_text  # 暂时返回原文，需要你自己实现转换逻辑\n",
    "\n",
    "# 读取JSON数据\n",
    "with open('/root/autodl-tmp/LLaMA-Factory/data/alpaca_gpt4_data_zh.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 随机选择50条数据不重复采样\n",
    "selected_data = random.sample(data, 3000)\n",
    "\n",
    "# 处理每一个条目\n",
    "for item in tqdm.tqdm(selected_data):\n",
    "    # 转换output字段\n",
    "    item['output'] = convert_to_classical_chinese(item['output'])\n",
    "\n",
    "# 保存新的JSON数据到另一个文件\n",
    "with open('/root/autodl-tmp/LLaMA-Factory/data/alpaca_gpt4_data_zh_modified.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(selected_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"转换完成，结果已保存。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# 假设这是转换到古文的函数\n",
    "def convert_to_classical_chinese(text):\n",
    "    # 这里应该是将现代文本转换为古文的逻辑\n",
    "    # 例如，你可以用替换规则或者调用某个外部的API进行转换\n",
    "    prompt = f\"把以下文字中的中文部分改写成三国演义原著的古文行文风格,不需要额外的输出和注解,一定不能重复输出：“{text}” \"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    res = chat_model.chat(messages)\n",
    "    return res[0].response_text  # 暂时返回原文，需要你自己实现转换逻辑\n",
    "\n",
    "# 读取JSON数据\n",
    "with open('/root/autodl-tmp/LLaMA-Factory/data/dpo_zh_demo.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "selected_data = random.sample(data, 10)\n",
    "\n",
    "# 逐条处理 selected_data 中每个元素的 'conversations'\n",
    "for item in tqdm.tqdm(selected_data):\n",
    "    # if 'conversations' in item:\n",
    "    #     for conversation in item['conversations']:\n",
    "    #         # 打印原始数据\n",
    "    #         print(\"原始数据: \", conversation['value'])\n",
    "    #         # 转换并打印转换后的数据\n",
    "    #         converted_value = convert_to_classical_chinese(conversation['value'])\n",
    "    #         print(\"转换后数据: \", converted_value)\n",
    "    #         # 更新数据\n",
    "    #         conversation['value'] = converted_value\n",
    "\n",
    "    # 处理 chosen 和 rejected，如果存在\n",
    "    if 'chosen' in item and 'value' in item['chosen']:\n",
    "        print(\"原始 chosen value: \", item['chosen']['value'])\n",
    "        item['chosen']['value'] = convert_to_classical_chinese(item['chosen']['value'])\n",
    "        print(\"转换后 chosen value: \", item['chosen']['value'])\n",
    "\n",
    "    if 'rejected' in item and 'value' in item['rejected']:\n",
    "        print(\"原始 rejected value: \", item['rejected']['value'])\n",
    "        item['rejected']['value'] = convert_to_classical_chinese(item['rejected']['value'])\n",
    "        print(\"转换后 rejected value: \", item['rejected']['value'])\n",
    "\n",
    "# 保存新的JSON数据到另一个文件\n",
    "with open('/root/autodl-tmp/LLaMA-Factory/data/XiYouji_DPO_modified.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(selected_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"转换完成，结果已保存。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 预热\n",
    "\n",
    "# prompt =\"严冬时节，鹅毛一样的大雪片在天空中到处飞舞着，有一个王后坐在王宫里的一扇窗子边，正在为她的女儿做针线活儿，寒风卷着雪片飘进了窗子，乌木窗台上飘落了不少雪花。\"  \n",
    "# prompt = \"把以下文字中的中文部分改写成三国演义原著的古文行文风格：“泰姬陵位于印度阿格拉市，是一座用白色大理石建造的壮丽陵墓，于1632年至1653年由蒙卧尔皇帝沙贾汗为纪念他逝世的妻子泰姬·玛哈尔而建。它被认为是蒙卧尔建筑的杰作之一，也是世界七大奇迹之一。整个建筑群包括一个清真寺、一座游客休息室、池塘和花园。泰姬陵不仅仅是一个壮观的建筑，它也象征着爱情的永恒和纯洁。每年都有成千上万的游客来到这里，欣赏它的美丽并参观其中的纪念品。” \" \n",
    "for i in range(10):\n",
    "    prompt_data=data[i]['output']\n",
    "    prompt = f\"把以下文字中的中文部分改写成三国演义原著的古文行文风格,不需要额外的输出,不能重复输出：“{prompt_data}” \" \n",
    "    print(prompt)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    res = chat_model.chat(messages)\n",
    "    print(\"chat reponse: \",res[0].response_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 预热\n",
    "\n",
    "# prompt =\"严冬时节，鹅毛一样的大雪片在天空中到处飞舞着，有一个王后坐在王宫里的一扇窗子边，正在为她的女儿做针线活儿，寒风卷着雪片飘进了窗子，乌木窗台上飘落了不少雪花。\"  \n",
    "prompt = \"我爱上她了，怎么办\" \n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "res = chat_model.chat(messages)\n",
    "print(\"chat reponse: \",res[0].response_text)\n",
    "\n",
    "prompt = res[0].response_text\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "res = style_model.chat(messages)\n",
    "print(\"style reponse: \", res[0].response_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
