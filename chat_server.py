import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
import sys
import fire
import gradio as gr
from gradio import utils
import torch
from peft import PeftModel
from transformers import GenerationConfig, AutoTokenizer, AutoModel
import yaml
import json
from loguru import logger
import time
import sys
from src.llamafactory.chat import ChatModel


if torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"

try:
    if torch.backends.mps.is_available():
        device = "mps"
except:
    pass



class Chat_Model(object):
    def __init__(self, scene=None, character=None, config_path="/root/autodl-tmp/LLaMA-Factory/roleplay/chat_model.yaml"):
        self.device = device
        self.scene = scene
        self.config_path = config_path
        self.character = character
        self.model = self.prepare_model(self.config_path)

    def prepare_model(self, config_path):
        # load model
        with open(config_path, 'r', encoding='utf-8') as f:
            chat_model_param = yaml.safe_load(f)
        chat_model = ChatModel(chat_model_param)
        return chat_model

    def get_headers(self):
        if self.scene and self.character:
            # headers = [
            #     {"role": "user", "content": f"è¯·æ‰®æ¼”{self.scene}ä¸­çš„{self.character}ã€‚ä½¿ç”¨ç°ä»£æ–‡é£æ™®é€šè¯å›è¯ï¼Œä¸è¦å¤æ–‡é£æ ¼ã€‚"},
            # ]
            headers = [
                {"role": "user", "content": f"ä½ æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚"},
            ]
        else:
            headers = []
        return headers

    def generate(self, messages, temperature=0.9, top_p=0.75, top_k=40, num_beams=1, do_sample=True, max_new_tokens=128):
        conversation = self.get_headers() + messages
        chat_model_res = self.model.chat(
            messages=conversation,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            num_beams=num_beams,
            do_sample=do_sample,
            return_dict_in_generate=True,
            output_scores=True,
            max_new_tokens=max_new_tokens,
        )
        return chat_model_res


def main(
    chat_model_config_path: str = "/root/autodl-tmp/LLaMA-Factory/roleplay/chat_model.yaml",
    style_model_config_path: str = "/root/autodl-tmp/LLaMA-Factory/roleplay/style_model.yaml",
    server_name: str = "0.0.0.0",
    share_gradio: bool = False,
):

    chat_model = Chat_Model(scene='ä¸‰å›½æ¼”ä¹‰', character="åˆ˜å¤‡", config_path=chat_model_config_path)
    with open(style_model_config_path, 'r', encoding='utf-8') as f:
        style_model_param = yaml.safe_load(f)

    style_model = ChatModel(style_model_param)

    def evaluate(
        input_text,
        chat_model_do_sample,
        chat_model_temperature,
        chat_model_top_p,
        chat_model_top_k,
        chat_model_num_beams,
        chat_model_max_new_tokens,
        chat_model_max_history,
        style_model_do_sample,
        style_model_temperature,
        style_model_top_p,
        style_model_top_k,
        style_model_num_beams,
        style_model_max_new_tokens,
        style_model_max_history,
        chat_model_history=[],
        style_model_history=[],
        **kwargs,
    ):
        # å‡è®¾æ‚¨æœ‰ chat_model å’Œ style_model è¿™ä¸¤ä¸ªæ¨¡å‹çš„å®ä¾‹
        context = input_text
        if chat_model_max_history > 0 and chat_model_history:
            temp_history = sum(chat_model_history, [])
            context = "".join(temp_history[-chat_model_max_history:]) + input_text

        chat_model_prompt = context
        messages = [{"role": "user", "content": chat_model_prompt}]
        chat_model_res = chat_model.generate(
            messages,
            temperature=chat_model_temperature,
            top_p=chat_model_top_p,
            top_k=chat_model_top_k,
            num_beams=chat_model_num_beams,
            do_sample=chat_model_do_sample,
            max_new_tokens=chat_model_max_new_tokens,
        )

        style_model_prompt = chat_model_res[0].response_text
        messages = [{"role": "user", "content": style_model_prompt}]
        style_model_res = style_model.chat(
            messages,
            temperature=style_model_temperature,
            top_p=style_model_top_p,
            top_k=style_model_top_k,
            num_beams=style_model_num_beams,
            do_sample=style_model_do_sample,
            return_dict_in_generate=True,
            output_scores=True,
            max_new_tokens=style_model_max_new_tokens,
        )

        output = style_model_res[0].response_text
        # style_model_prompt = '1111111'
        # output = 'dfsfdfs'
        chat_model_history.append([input_text,style_model_prompt])
        style_model_history.append([input_text, output])
        return "", chat_model_history,style_model_history

    with gr.Blocks() as demo:
        title = "ğŸ¤–Style-LLM-ChatğŸ¤–"
        description = """
        Style-LLM-Chat æ˜¯ä½¿ç”¨LLAMA-FACTORYæ¡†æ¶ä¸Šé‡‡ç”¨è‡ªå®šä¹‰å¤æ–‡é£æ ¼Â·æ•°æ®é›†,ä½¿ç”¨SFT,RM,PPOè¿›è¡Œå¾®è°ƒ,èƒ½å¤Ÿå®ç°å¤æ–‡é£æ ¼çš„å¯¹è¯ã€‚
        é¡¹ç›®æ›´å¤šè¯¦æƒ…è§ â€œhttps://github.com/YANGCHEN205/style-llm-chatâ€
        å‚æ•°è¯¦è§£ï¼š
        temperatureï¼ˆæ¸©åº¦ï¼‰: è¿™ä¸ªå‚æ•°ç”¨äºæ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§ã€‚æ¸©åº¦è¾ƒä½ï¼ˆæ¯”å¦‚0.1ï¼‰ä¼šä½¿æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬æ¯”è¾ƒç¡®å®šæ€§å’Œä¸€è‡´æ€§å¼ºï¼Œè€Œè¾ƒé«˜çš„æ¸©åº¦ï¼ˆæ¯”å¦‚1.0æˆ–æ›´é«˜ï¼‰ä¼šä½¿æ–‡æœ¬æ›´å¤šæ ·åŒ–å’Œéšæœºã€‚
        top_pï¼ˆä¹Ÿç§°ä½œnucleus samplingï¼‰: è¿™ä¸ªå‚æ•°ç”¨äºæ§åˆ¶åœ¨ç”Ÿæˆæ¯ä¸€ä¸ªè¯æ—¶è€ƒè™‘çš„æ¦‚ç‡è´¨é‡ã€‚å…·ä½“æ¥è¯´ï¼Œtop_pä¸º0.9æ„å‘³ç€åªè€ƒè™‘ç´¯è®¡æ¦‚ç‡è‡³å°‘è¾¾åˆ°90%çš„è¯ã€‚è¿™æœ‰åŠ©äºå‰”é™¤é‚£äº›æå…¶ç½•è§çš„è¯ï¼Œä½¿ç”Ÿæˆçš„å†…å®¹æ›´åŠ æµç•…å’Œåˆç†ã€‚
        top_k: è¿™ä¸ªå‚æ•°é™åˆ¶äº†åœ¨ç”Ÿæˆæ¯ä¸ªè¯æ—¶è€ƒè™‘çš„å¯èƒ½è¯çš„æ•°é‡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœtop_kè®¾ä¸º50ï¼Œåˆ™æ¯æ¬¡ç”Ÿæˆè¯æ—¶åªä»æ¦‚ç‡æœ€é«˜çš„å‰50ä¸ªè¯ä¸­é€‰æ‹©ã€‚
        num_beamsï¼ˆæŸæœç´¢ï¼‰: è¿™ä¸ªå‚æ•°ç”¨äºæŸæœç´¢ï¼Œä¸€ç§ç”¨äºç”Ÿæˆæ›´åˆç†æ–‡æœ¬çš„æŠ€æœ¯ã€‚num_beamsè®¾ç½®ä¸ºå¤§äº1çš„å€¼æ—¶ï¼Œæ¨¡å‹å°†æ¢ç´¢å¤šç§å¯èƒ½çš„å¥å­ç»„åˆï¼Œé€‰æ‹©æ•´ä½“ä¸Šæœ€æœ‰å¯èƒ½ï¼ˆæˆ–å¾—åˆ†æœ€é«˜ï¼‰çš„è¾“å‡ºã€‚
        do_sample: è¿™ä¸ªå¸ƒå°”å‚æ•°ç¡®å®šæ˜¯å¦åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶è¿›è¡Œéšæœºé‡‡æ ·ã€‚å¦‚æœè®¾ç½®ä¸ºTrueï¼Œæ¨¡å‹åœ¨ç”Ÿæˆæ¯ä¸ªè¯æ—¶ä¼šåŸºäºä¿®æ”¹åçš„æ¦‚ç‡åˆ†å¸ƒè¿›è¡ŒæŠ½æ ·ï¼Œå¢åŠ æ–‡æœ¬çš„å¤šæ ·æ€§ã€‚
        return_dict_in_generate: å½“è¿™ä¸ªå‚æ•°ä¸ºTrueæ—¶ï¼Œæ¨¡å‹ç”Ÿæˆå‡½æ•°å°†è¿”å›ä¸€ä¸ªåŒ…å«æ›´å¤šä¿¡æ¯çš„å­—å…¸ï¼Œå¦‚ç”Ÿæˆçš„è¯çš„åˆ†æ•°ç­‰ï¼Œè€Œä¸ä»…ä»…æ˜¯æ–‡æœ¬ã€‚
        output_scores: å¦‚æœè®¾ç½®ä¸ºTrueï¼Œç”Ÿæˆçš„è¾“å‡ºå°†åŒ…æ‹¬é¢„æµ‹æ¯ä¸ªè¯çš„åˆ†æ•°ï¼ˆé€šå¸¸æ˜¯æ¦‚ç‡å¯¹æ•°ï¼‰ã€‚è¿™å¯¹äºç†è§£å’Œåˆ†ææ¨¡å‹çš„è¡Œä¸ºéå¸¸æœ‰ç”¨ã€‚
        max_new_tokens: è¿™ä¸ªå‚æ•°é™åˆ¶äº†ç”Ÿæˆçš„æœ€å¤§è¯ï¼ˆtokenï¼‰æ•°é‡ã€‚è¿™æœ‰åŠ©äºæ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„é•¿åº¦ã€‚
        ğŸ“–é£æ ¼æ¨¡å‹ï¼šè¥¿æ¸¸è®°                   ğŸ‘¨â€ğŸ¦²è§’è‰²ï¼šåˆ˜å¤‡
        ğŸ“•å¯¹è¯æ¨¡å‹:Qwen2-7B-Instruct         ğŸ‘¨â€ğŸ¦²è§’è‰²ï¼šåˆ˜å¤‡
        """
        gr.Markdown("<h1 style='text-align: center; margin-bottom: 1rem'>" + title + "</h1>")
        gr.Markdown(description)

        with gr.Row():
            with gr.Column(scale=2):
                chat_model_chatbot = gr.Chatbot(label="chat_model_å¯¹è¯æ¡†")


            with gr.Column(scale=1):
                chat_model_do_sample = gr.Checkbox(label="Do sample")
                chat_model_temperature = gr.Slider(minimum=0, maximum=1, value=0.9, label="Temperature")
                chat_model_top_p = gr.Slider(minimum=0, maximum=1, value=0.75, label="Top p")
                chat_model_top_k = gr.Slider(minimum=0, maximum=100, step=1, value=40, label="Top k")
                chat_model_num_beams = gr.Slider(minimum=1, maximum=10, step=1, value=1, label="Beams")
                chat_model_max_new_tokens = gr.Slider(minimum=1, maximum=512, step=1, value=64, label="Max tokens")
                chat_model_max_history = gr.Slider(minimum=0, maximum=20, step=1, value=10, label="Max history")

            with gr.Column(scale=2):
                style_model_chatbot = gr.Chatbot(label="style_model_å¯¹è¯æ¡†")

            with gr.Column(scale=1):
                style_model_do_sample = gr.Checkbox(label="Do sample")
                style_model_temperature = gr.Slider(minimum=0, maximum=1, value=0.9, label="Temperature")
                style_model_top_p = gr.Slider(minimum=0, maximum=1, value=0.75, label="Top p")
                style_model_top_k = gr.Slider(minimum=0, maximum=100, step=1, value=40, label="Top k")
                style_model_num_beams = gr.Slider(minimum=1, maximum=10, step=1, value=1, label="Beams")
                style_model_max_new_tokens = gr.Slider(minimum=1, maximum=512, step=1, value=64, label="Max tokens")
                style_model_max_history = gr.Slider(minimum=0, maximum=20, step=1, value=10, label="Max history")
            with gr.Column(scale=1):
                chat_model_msg = gr.Textbox(label="è¾“å…¥æ¡†")
                
            submit = gr.Button("æäº¤", variant="primary")
            clear = gr.Button("æ¸…é™¤")
            retry = gr.Button("é‡è¯•")

            submit.click(
                evaluate,
                [
                    chat_model_msg,
                    chat_model_do_sample,
                    chat_model_temperature,
                    chat_model_top_p,
                    chat_model_top_k,
                    chat_model_num_beams,
                    chat_model_max_new_tokens,
                    chat_model_max_history,
                    style_model_do_sample,
                    style_model_temperature,
                    style_model_top_p,
                    style_model_top_k,
                    style_model_num_beams,
                    style_model_max_new_tokens,
                    style_model_max_history,
                    chat_model_chatbot,
                    style_model_chatbot
                ],
                [chat_model_msg, chat_model_chatbot, style_model_chatbot], queue=False
            )

            clear.click(lambda: (None, None,None), None, [chat_model_chatbot, style_model_chatbot], queue=False)
            retry.click(
    lambda v1, v2: (v1[-1][0] if v1 else "", v1[:-1], v2[-1][0] if v2 else ""),  # ç¬¬ä¸‰ä¸ªè¾“å‡º
    inputs=[chat_model_chatbot],
    outputs=[chat_model_msg, chat_model_chatbot, style_model_chatbot],
    queue=False
)

    demo.title = title
    demo.launch(server_name=server_name, share=share_gradio)


if __name__ == "__main__":
    fire.Fire(main)
