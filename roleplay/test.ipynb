{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c3f522da2344088ac84d0ff154c4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40822e0e29d4edeaef9ea1ebff34e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:  59%|#####9    | 2.11G/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9950cc056d94974952d779725982cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f6eb81dd340>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e5390a39614b4ea62770240bca2453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'01-ai/Yi-6b'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\", trust_remote_code=True,cache_dir=\"/root/autodl-tmp/hug/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-7B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_8bit=False,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=\"/root/autodl-tmp/hug/\"\n",
    ")\n",
    "\n",
    "\"01-ai/Yi-1.5-6B-Chat\"\n",
    "\"Qwen/Qwen2-7B-Instruct\"\n",
    "\"Qwen/Qwen2-0.5B-Instruct\"\n",
    "\"Qwen/Qwen2-0.5B\"\n",
    "\"Qwen/Qwen1.5-0.5B\"\n",
    "\"Qwen/Qwen1.5-0.5B-chat\"\n",
    "\"Qwen/Qwen1.5-7B-chat\"\n",
    "\"Qwen/Qwen-7B-chat\"\n",
    "\"Qwen/Qwen-0.5B-chat\"\n",
    "\"THUDM/chatglm3-6b\"\n",
    "\"01-ai/Yi-6b\"\n",
    "# export HF_HOME=\"/root/autodl-tmp/hug\"\n",
    "# export HF_ENDPOINT=\"https://hf-mirror.com\"\n",
    "# https://hf-mirror.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.convert_tokens_to_ids(\"<bos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from transformers import AutoModel\n",
    "# import torch\n",
    "# from peft import PeftModel\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm3-6b\", trust_remote_code=True,cache_dir=\"/root/autodl-tmp/hug/\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"THUDM/chatglm3-6b\",\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "#     load_in_8bit=False,\n",
    "#     trust_remote_code=True,\n",
    "#     cache_dir=\"/root/autodl-tmp/hug/\"\n",
    "# )\n",
    "# \"Qwen/Qwen1.5-0.5B\"\n",
    "# \"THUDM/chatglm3-6b\"\n",
    "# \"01-ai/Yi-6b\"\n",
    "# # export HF_HOME=\"/root/autodl-tmp/hug\"\n",
    "# # export HF_ENDPOINT=\"https://hf-mirror.com\"\n",
    "# # https://hf-mirror.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llamafactory-cli chat /root/autodl-tmp/LLaMA-Factory/examples/roleplay/qwen_sft_predict.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from safetensors.torch import load_file\n",
    "# from peft import PeftModel\n",
    "\n",
    "# # 加载 PeftModel\n",
    "# model = PeftModel.from_pretrained(model, \"/root/autodl-tmp/LLaMA-Factory/output/sft\")\n",
    "\n",
    "# # 使用 safetensors 库加载 safetensor 格式的权重\n",
    "# checkpoint = load_file(\"/root/autodl-tmp/LLaMA-Factory/output/sft/adapter_model.safetensors\", device=\"cuda\")\n",
    "\n",
    "# # 将预训练权重加载到模型中，并捕获 missing_keys 和 unexpected_keys\n",
    "# missing_keys, unexpected_keys = model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "# # 输出缺失的键\n",
    "# print(\"Missing keys:\", missing_keys)\n",
    "# print(\"Unexpected keys:\", unexpected_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoints_to_merge=['/root/autodl-tmp/LLaMA-Factory/output/sft']\n",
    "# for checkpoint in checkpoints_to_merge:\n",
    "#     model = PeftModel.from_pretrained(model, checkpoint)\n",
    "#     # model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Qwen2ForCausalLM' object has no attribute 'chat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 2\u001b[0m response, history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m(tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m你是谁\u001b[39m\u001b[38;5;124m\"\u001b[39m, history\u001b[38;5;241m=\u001b[39m[]) \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Qwen2ForCausalLM' object has no attribute 'chat'"
     ]
    }
   ],
   "source": [
    "model = model.eval()\n",
    "response, history = model.chat(tokenizer, \"你是谁\", history=[]) \n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "你是谁<|im_end|>\n",
      "\n",
      "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847, 151645,\n",
      "            198, 151644,    872,    198, 105043, 100165, 151645,    198]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "assistant\n",
      "答案：我是一只小蚂蚁，我住在一片茂密的森林里，森林里住着许多小动物，有小松鼠、小兔子、小猴子、小青蛙、小蛇、小鸭子、小鱼、小虾、小螃蟹、小乌龟、小青蛙、小鱼、小虾、小螃蟹、小乌龟、小青蛙、小鱼、小虾、小螃蟹、小乌龟、小青蛙、小鱼、小虾、小螃蟹、小乌龟、小青蛙、小鱼、小虾、小螃蟹、小乌龟、小青蛙\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"严冬时节，鹅毛一样的大雪片在天空中到处飞舞着，有一个王后坐在王宫里的一扇窗子边，正在为她的女儿做针线活儿，寒风卷着雪片飘进了窗子，乌木窗台上飘落了不少雪花。\" \n",
    "prompt = \"你是谁\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "# text = prompt\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    # add_generation_prompt=True\n",
    ")\n",
    "print(text)\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
    "print(model_inputs)\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=128\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant\n",
      "答案：我是一只小蚂蚁，我住在一片茂密的森林里，森林里住着许多小动物，有小松鼠、小兔子、小猴子、小青蛙、小蛇、小鸭子、小鱼、小虾、小螃蟹、小乌龟、小青蛙、小鱼、小虾、小螃蟹、小乌龟、小青蛙、小鱼、小虾、小螃蟹、小乌龟、小青蛙、小鱼、小虾、小螃蟹、小乌龟、小青蛙、小鱼、小虾、小螃蟹、小乌龟、小青蛙\n"
     ]
    }
   ],
   "source": [
    "print(response )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
