{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2024/6/16 20:50\n",
    "# @Author  : yblir\n",
    "# @File    : lyb_lora_inference.py\n",
    "# explain  : \n",
    "# =======================================================\n",
    "import yaml\n",
    "import json\n",
    "from loguru import logger\n",
    "import time\n",
    "import sys\n",
    "from src.llamafactory.chat import ChatModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2106] 2024-08-30 12:48:53,263 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-08-30 12:48:53,263 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-08-30 12:48:53,264 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-08-30 12:48:53,264 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2106] 2024-08-30 12:48:53,265 >> loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:731] 2024-08-30 12:48:53,870 >> loading configuration file /root/autodl-tmp/hug/models--01-ai--Yi-1.5-6B-Chat/snapshots/15fc040a9c9a81098f05ded04e6e519ed91b4f37/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-08-30 12:48:53,872 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/root/autodl-tmp/hug/models--01-ai--Yi-1.5-6B-Chat/snapshots/15fc040a9c9a81098f05ded04e6e519ed91b4f37\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 5000000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 64000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/30/2024 12:48:53 - INFO - src.llamafactory.model.patcher - Using KV cache for faster generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3471] 2024-08-30 12:48:53,899 >> loading weights file /root/autodl-tmp/hug/models--01-ai--Yi-1.5-6B-Chat/snapshots/15fc040a9c9a81098f05ded04e6e519ed91b4f37/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1519] 2024-08-30 12:48:53,900 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:962] 2024-08-30 12:48:53,902 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198bbbffef0c49b5b570b80895c610d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:4280] 2024-08-30 12:49:39,952 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-08-30 12:49:39,953 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/autodl-tmp/hug/models--01-ai--Yi-1.5-6B-Chat/snapshots/15fc040a9c9a81098f05ded04e6e519ed91b4f37.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:915] 2024-08-30 12:49:39,957 >> loading configuration file /root/autodl-tmp/hug/models--01-ai--Yi-1.5-6B-Chat/snapshots/15fc040a9c9a81098f05ded04e6e519ed91b4f37/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-08-30 12:49:39,959 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/30/2024 12:49:39 - INFO - src.llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "08/30/2024 12:49:40 - INFO - src.llamafactory.model.adapter - Merged 1 adapter(s).\n",
      "08/30/2024 12:49:40 - INFO - src.llamafactory.model.adapter - Loaded adapter(s): /root/autodl-tmp/LLaMA-Factory/output/Yi-6B-chat/sft\n",
      "08/30/2024 12:49:40 - INFO - src.llamafactory.model.loader - all params: 6,061,035,520\n"
     ]
    }
   ],
   "source": [
    "with open('/root/autodl-tmp/LLaMA-Factory/roleplay/style_model.yaml', 'r', encoding='utf-8') as f:\n",
    "    param = yaml.safe_load(f)\n",
    "\n",
    "style_model = ChatModel(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "style reponse:   吾愿闻吾之主人之意愿，以供选择。\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"你今天想吃什么？\"    \n",
    "messages = [                                                                                                                                \n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "res = style_model.chat(messages)\n",
    "print(\"style reponse: \", res[0].response_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
